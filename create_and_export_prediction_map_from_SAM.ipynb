{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "13UEG_e3i0za9YJcP3Q2GlKQiqNYlbph8",
      "authorship_tag": "ABX9TyNYDZ+nEk4bhrmZ9nNgRC4m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "24da2b64f67b49e0bc733d2d0e05e95f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15f67844868d433daef82307dc8e9268",
              "IPY_MODEL_24f877602ca64d47b7a4baba92ff4037",
              "IPY_MODEL_9eee5f0cd6df4368b2573c95c761dce5"
            ],
            "layout": "IPY_MODEL_16e23000c3574dd2b47d0ad2f3a54523"
          }
        },
        "15f67844868d433daef82307dc8e9268": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_801ea980872b4f53aca6ba1dda1af853",
            "placeholder": "​",
            "style": "IPY_MODEL_af5892799e9946bb84a30cfda34a9cf9",
            "value": "config.json: 100%"
          }
        },
        "24f877602ca64d47b7a4baba92ff4037": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57ad696b2fcc455baf5f95adc76b123c",
            "max": 6566,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_185f0eb7b2b34b95916ce4410afb0491",
            "value": 6566
          }
        },
        "9eee5f0cd6df4368b2573c95c761dce5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c29716278c9346388f099c1b9bb1b91e",
            "placeholder": "​",
            "style": "IPY_MODEL_21982154cbfa4e1082655214872aed0a",
            "value": " 6.57k/6.57k [00:00&lt;00:00, 552kB/s]"
          }
        },
        "16e23000c3574dd2b47d0ad2f3a54523": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "801ea980872b4f53aca6ba1dda1af853": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af5892799e9946bb84a30cfda34a9cf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57ad696b2fcc455baf5f95adc76b123c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "185f0eb7b2b34b95916ce4410afb0491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c29716278c9346388f099c1b9bb1b91e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21982154cbfa4e1082655214872aed0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f4398a571f94bb0b6a1f65567c6fc63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e6c12036e6444ec294e207f4e429212d",
              "IPY_MODEL_10cf999eded64f218e33e467d0bca43e",
              "IPY_MODEL_d94027d2031e4824b6a2fb95abd560b0"
            ],
            "layout": "IPY_MODEL_fab70bc88ad54bc1badaff1c5a156779"
          }
        },
        "e6c12036e6444ec294e207f4e429212d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10e3dc0d30b940cc9255ee97b1e700b6",
            "placeholder": "​",
            "style": "IPY_MODEL_188f060151814abd9ae7d581f246bc11",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "10cf999eded64f218e33e467d0bca43e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7852ada0a7846b1a55b536f48e17855",
            "max": 466,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40efb74236ea413bb8710ab5730af129",
            "value": 466
          }
        },
        "d94027d2031e4824b6a2fb95abd560b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fa7c3fd00f54307b298540a4fd8eac8",
            "placeholder": "​",
            "style": "IPY_MODEL_d23a0665262240eb9b416cfcdc68c179",
            "value": " 466/466 [00:00&lt;00:00, 39.6kB/s]"
          }
        },
        "fab70bc88ad54bc1badaff1c5a156779": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10e3dc0d30b940cc9255ee97b1e700b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "188f060151814abd9ae7d581f246bc11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7852ada0a7846b1a55b536f48e17855": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40efb74236ea413bb8710ab5730af129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7fa7c3fd00f54307b298540a4fd8eac8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d23a0665262240eb9b416cfcdc68c179": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leia2000/Bachelorarbeit_Skripte/blob/main/create_and_export_prediction_map_from_SAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFCzKWzKYBUT"
      },
      "outputs": [],
      "source": [
        "!pip install rasterio\n",
        "!pip install numpy\n",
        "!pip install os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries\n",
        "#SAM\n",
        "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
        "#Transformers\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "#Datasets to prepare data and monai if you want to use special loss functions\n",
        "!pip install datasets\n",
        "!pip install -q monai\n",
        "#Patchify to divide large images into smaller patches for training. (Not necessary for smaller images)\n",
        "!pip install patchify"
      ],
      "metadata": {
        "id": "884ZwxDNbXRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import Dataset\n",
        "from PIL import Image\n",
        "import tifffile\n",
        "from patchify import patchify  #Only to handle large images\n",
        "import random\n",
        "from scipy import ndimage\n",
        "from transformers import SamModel, SamConfig, SamProcessor\n",
        "import torch"
      ],
      "metadata": {
        "id": "wU8EBH0DbRhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory containing TIF images\n",
        "dir_path = '/content/drive/MyDrive/Bachelorarbeit_Hannes_Grünbeck/TIF/Quadrate'\n",
        "\n",
        "# List all TIF files in the directory\n",
        "tif_files = [os.path.join(dir_path, f) for f in os.listdir(dir_path) if f.endswith('.tif')]\n",
        "\n",
        "# Initialize an empty list to hold the arrays\n",
        "image_stack = []\n",
        "metadata_list = []\n",
        "# Total number of files\n",
        "total_files = len(tif_files)\n",
        "# Loop through each file, read the data, and stack\n",
        "\n",
        "for idx, tif_file in enumerate(tif_files, start=1):\n",
        "    print(f\"Processing file {idx}/{total_files}: {os.path.basename(tif_file)}\")\n",
        "    with rasterio.open(tif_file) as src:\n",
        "        data = src.read(1)  # Read the first band\n",
        "        image_stack.append(data)\n",
        "        metadata_list.append(src.meta)  # Save metadata\n",
        "\n",
        "# Convert list to numpy array (stacked)\n",
        "#stacked_array = np.stack(image_stack, axis=0)"
      ],
      "metadata": {
        "id": "znoL4C9TgJCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory containing TIF images\n",
        "dir_path = '/content/drive/MyDrive/Bachelorarbeit_Hannes_Grünbeck/TIF/Quadrate'\n",
        "\n",
        "# List all TIF files in the directory\n",
        "tif_files = [os.path.join(dir_path, f) for f in os.listdir(dir_path) if f.endswith('.tif')]\n",
        "\n",
        "# Initialize an empty list to hold the arrays\n",
        "image_stack = []\n",
        "metadata_list = []\n",
        "# Total number of files\n",
        "total_files = len(tif_files)\n",
        "\n",
        "# Target shape (lowest shape)\n",
        "target_shape = (6538, 6754)\n",
        "\n",
        "# Loop through each file, read the data, and stack\n",
        "for idx, tif_file in enumerate(tif_files, start=1):\n",
        "    print(f\"Processing file {idx}/{total_files}: {os.path.basename(tif_file)}\")\n",
        "    with rasterio.open(tif_file) as src:\n",
        "        data = src.read(1)  # Read the first band\n",
        "\n",
        "        # Cut the image to the target shape\n",
        "        data = data[:target_shape[1], :target_shape[0]]\n",
        "\n",
        "        image_stack.append(data)\n",
        "        metadata_list.append(src.meta)  # Save metadata\n",
        "\n",
        "# Convert list to numpy array (stacked)\n",
        "stacked_array = np.stack(image_stack, axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDitukQ1YcCS",
        "outputId": "0073d057-171d-48e7-eddb-c9cce364d53b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 1/4: oben_links.tif\n",
            "Processing file 2/4: unten_links.tif\n",
            "Processing file 3/4: unten_rechts.tif\n",
            "Processing file 4/4: oben_rechts.tif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the dimensions of each image in the stack\n",
        "for i, image in enumerate(image_stack):\n",
        "    height, width = image.shape  # Assuming the images are 2D\n",
        "    print(f\"Image {i + 1}: Width = {width}, Height = {height}\")"
      ],
      "metadata": {
        "id": "eoEuFp_sZoEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f99d4860-f9e7-4b81-a9dd-e1bbbd7f723b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 1: Width = 6538, Height = 6754\n",
            "Image 2: Width = 6538, Height = 6754\n",
            "Image 3: Width = 6538, Height = 6754\n",
            "Image 4: Width = 6538, Height = 6754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_array = np.stack(image_stack, axis=0)"
      ],
      "metadata": {
        "id": "nYh8e0HhYmrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get bounding boxes from mask.\n",
        "def get_bounding_box(ground_truth_map):\n",
        "  # get bounding box from mask\n",
        "  y_indices, x_indices = np.where(ground_truth_map > 0)\n",
        "  x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
        "  y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
        "  # add perturbation to bounding box coordinates\n",
        "  H, W = ground_truth_map.shape\n",
        "  x_min = max(0, x_min - np.random.randint(0, 20))\n",
        "  x_max = min(W, x_max + np.random.randint(0, 20))\n",
        "  y_min = max(0, y_min - np.random.randint(0, 20))\n",
        "  y_max = min(H, y_max + np.random.randint(0, 20))\n",
        "  bbox = [x_min, y_min, x_max, y_max]\n",
        "\n",
        "  return bbox"
      ],
      "metadata": {
        "id": "z4wNbjLBbRgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model configuration\n",
        "model_config = SamConfig.from_pretrained(\"facebook/sam-vit-base\")\n",
        "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
        "\n",
        "# Create an instance of the model architecture with the loaded configuration\n",
        "my_mito_model = SamModel(config=model_config)\n",
        "#Update the model by loading the weights from saved file.\n",
        "my_mito_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Bachelorarbeit_Hannes_Grünbeck/Ergebnisse_SAM/SAM_Twigs.pth\"))"
      ],
      "metadata": {
        "id": "CnJKA3e6bRd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257,
          "referenced_widgets": [
            "24da2b64f67b49e0bc733d2d0e05e95f",
            "15f67844868d433daef82307dc8e9268",
            "24f877602ca64d47b7a4baba92ff4037",
            "9eee5f0cd6df4368b2573c95c761dce5",
            "16e23000c3574dd2b47d0ad2f3a54523",
            "801ea980872b4f53aca6ba1dda1af853",
            "af5892799e9946bb84a30cfda34a9cf9",
            "57ad696b2fcc455baf5f95adc76b123c",
            "185f0eb7b2b34b95916ce4410afb0491",
            "c29716278c9346388f099c1b9bb1b91e",
            "21982154cbfa4e1082655214872aed0a",
            "0f4398a571f94bb0b6a1f65567c6fc63",
            "e6c12036e6444ec294e207f4e429212d",
            "10cf999eded64f218e33e467d0bca43e",
            "d94027d2031e4824b6a2fb95abd560b0",
            "fab70bc88ad54bc1badaff1c5a156779",
            "10e3dc0d30b940cc9255ee97b1e700b6",
            "188f060151814abd9ae7d581f246bc11",
            "d7852ada0a7846b1a55b536f48e17855",
            "40efb74236ea413bb8710ab5730af129",
            "7fa7c3fd00f54307b298540a4fd8eac8",
            "d23a0665262240eb9b416cfcdc68c179"
          ]
        },
        "outputId": "f36bec02-9bb8-4b32-9299-50f6fdd62fa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/6.57k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24da2b64f67b49e0bc733d2d0e05e95f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f4398a571f94bb0b6a1f65567c6fc63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-f46dcd68edac>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  my_mito_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Bachelorarbeit_Hannes_Grünbeck/Ergebnisse_SAM/SAM_Twigs.pth\"))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set the device to cuda if available, otherwise use cpu\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "my_mito_model.to(device)"
      ],
      "metadata": {
        "id": "q08lpIHubRbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply a trained model on large image\n",
        "#large_test_images = tifffile.imread(\"/content/drive/MyDrive/Bachelorarbeit_Hannes_Grünbeck/TIF/ortho_plot_8_230726_modified_georef.tif\")\n",
        "large_test_images = stacked_array\n",
        "large_test_image = large_test_images[0]\n",
        "patches = patchify(large_test_image, (256, 256), step=256)  #Step=256 for 256 patches means no overlap"
      ],
      "metadata": {
        "id": "EhPRMFKzbRZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "input_points (torch.FloatTensor of shape (batch_size, num_points, 2)) —\n",
        "Input 2D spatial points, this is used by the prompt encoder to encode the prompt.\n",
        "Generally yields to much better results. The points can be obtained by passing a\n",
        "list of list of list to the processor that will create corresponding torch tensors\n",
        "of dimension 4. The first dimension is the image batch size, the second dimension\n",
        "is the point batch size (i.e. how many segmentation masks do we want the model to\n",
        "predict per input point), the third dimension is the number of points per segmentation\n",
        "mask (it is possible to pass multiple points for a single mask), and the last dimension\n",
        "is the x (vertical) and y (horizontal) coordinates of the point. If a different number\n",
        "of points is passed either for each image, or for each mask, the processor will create\n",
        "“PAD” points that will correspond to the (0, 0) coordinate, and the computation of the\n",
        "embedding will be skipped for these points using the labels.\n",
        "\n",
        "\"\"\"\n",
        "# Define the size of your array\n",
        "array_size = 256\n",
        "\n",
        "# Define the size of your grid\n",
        "grid_size = 10\n",
        "\n",
        "# Generate the grid points\n",
        "x = np.linspace(0, array_size-1, grid_size)\n",
        "y = np.linspace(0, array_size-1, grid_size)\n",
        "\n",
        "# Generate a grid of coordinates\n",
        "xv, yv = np.meshgrid(x, y)\n",
        "\n",
        "# Convert the numpy arrays to lists\n",
        "xv_list = xv.tolist()\n",
        "yv_list = yv.tolist()\n",
        "\n",
        "# Combine the x and y coordinates into a list of list of lists\n",
        "input_points = [[[int(x), int(y)] for x, y in zip(x_row, y_row)] for x_row, y_row in zip(xv_list, yv_list)]\n",
        "\n",
        "#We need to reshape our nxn grid to the expected shape of the input_points tensor\n",
        "# (batch_size, point_batch_size, num_points_per_image, 2),\n",
        "# where the last dimension of 2 represents the x and y coordinates of each point.\n",
        "#batch_size: The number of images you're processing at once.\n",
        "#point_batch_size: The number of point sets you have for each image.\n",
        "#num_points_per_image: The number of points in each set.\n",
        "input_points = torch.tensor(input_points).view(1, 1, grid_size*grid_size, 2)"
      ],
      "metadata": {
        "id": "g5CFSHaYbRW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.array(input_points).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xa0V8f3dbRSf",
        "outputId": "c1a5f14d-cf13-4f6b-d71d-d68ee3cde4be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 1, 100, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patches.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeIBL1UJbRJT",
        "outputId": "77e739b5-597d-4dcc-da2b-af5db35b6614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26, 25, 256, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a random patch for segmentation\n",
        "\n",
        "# Compute the total number of 256x256 arrays\n",
        "num_arrays = patches.shape[0] * patches.shape[1]\n",
        "# Select a random index\n",
        "index = np.random.choice(num_arrays)\n",
        "# Compute the indices in the original array\n",
        "#i = index // patches.shape[1]\n",
        "#j = index % patches.shape[1]\n",
        "\n",
        "#Or pick a specific patch for study.\n",
        "i, j = 0,0\n",
        "\n",
        "# Selectelected patch for segmentation\n",
        "random_array = patches[i, j]\n",
        "\n",
        "\n",
        "single_patch = Image.fromarray(random_array)"
      ],
      "metadata": {
        "id": "0bw7ltf3bQig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare image for the model\n",
        "\n",
        "#First try without providing any prompt (no bounding box or input_points)\n",
        "#Convert to RGB for the correct number dimensions\n",
        "#inputs = processor(single_patch.convert(\"RGB\"), return_tensors=\"pt\")\n",
        "#Now try with bounding boxes. Remember to uncomment.\n",
        "#Convert to RGB for the correct number dimensions\n",
        "inputs = processor(single_patch.convert(\"RGB\"), input_points=input_points, return_tensors=\"pt\") # Convert to RGB before passing input_points\n",
        "# Move the input tensor to the GPU if it's not already there\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "my_mito_model.eval()\n",
        "# forward pass\n",
        "with torch.no_grad():\n",
        "  outputs = my_mito_model(**inputs, multimask_output=False)\n",
        "# apply sigmoid\n",
        "single_patch_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
        "# convert soft mask to hard mask\n",
        "single_patch_prob = single_patch_prob.cpu().numpy().squeeze()\n",
        "single_patch_prediction = (single_patch_prob > 0.5).astype(np.uint8)"
      ],
      "metadata": {
        "id": "jqSAkdr7bQwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18a7b18a-5b5a-4a93-973b-9bb4aac482d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/image_processing_utils.py:41: UserWarning: The following named arguments are not valid for `SamImageProcessor.preprocess` and were ignored: 'point_pad_value'\n",
            "  return self.preprocess(images, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Plot the first image on the left\n",
        "axes[0].imshow(np.array(single_patch), cmap='gray')  # Assuming the first image is grayscale\n",
        "axes[0].set_title(\"Image\")\n",
        "\n",
        "# Plot the second image on the right\n",
        "axes[1].imshow(single_patch_prob)  # Assuming the second image is grayscale\n",
        "axes[1].set_title(\"Probability Map\")\n",
        "\n",
        "# Plot the second image on the right\n",
        "axes[2].imshow(single_patch_prediction, cmap='gray')  # Assuming the second image is grayscale\n",
        "axes[2].set_title(\"Mask\")\n",
        "\n",
        "# Hide axis ticks and labels\n",
        "for ax in axes:\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "\n",
        "# Display the images side by side\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k8hGTfYFb-FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "from rasterio.transform import Affine\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "###########################################\n",
        "###  Iterate and export img, mask and   ###\n",
        "###  prediction for ONE items in stack  ###\n",
        "###########################################\n",
        "\n",
        "\n",
        "# Get CRS data from the original image metadata\n",
        "crs = metadata_list[0]['crs']\n",
        "transform = metadata_list[0]['transform']\n",
        "height, width = large_test_image.shape\n",
        "\n",
        "\n",
        "# Initialize full-size arrays for probability map and prediction mask\n",
        "prob_map_full = np.zeros_like(large_test_image, dtype=np.float32)  # Use appropriate data type\n",
        "pred_mask_full = np.zeros_like(large_test_image, dtype=np.uint8)  # Use appropriate data type\n",
        "\n",
        "# Function to update the full maps with a patch\n",
        "def update_full_maps(patch_prob, patch_mask, row, col):\n",
        "    prob_map_full[row * 256:(row + 1) * 256, col * 256:(col + 1) * 256] = patch_prob\n",
        "    pred_mask_full[row * 256:(row + 1) * 256, col * 256:(col + 1) * 256] = patch_mask\n",
        "\n",
        "# Iterate through all patches and make predictions\n",
        "for i in range(patches.shape[0]):\n",
        "    for j in range(patches.shape[1]):\n",
        "        random_array = patches[i, j]\n",
        "        single_patch = Image.fromarray(random_array)\n",
        "\n",
        "        # Model prediction for the current patch\n",
        "        inputs = processor(single_patch.convert(\"RGB\"), input_points=input_points, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        my_mito_model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = my_mito_model(**inputs, multimask_output=False)\n",
        "\n",
        "        # Process outputs to get probability map and prediction mask for the patch\n",
        "        single_patch_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
        "        single_patch_prob = single_patch_prob.cpu().numpy().squeeze()\n",
        "        single_patch_prediction = (single_patch_prob > 0.5).astype(np.uint8)\n",
        "\n",
        "        # Update the full-size maps with the patch results\n",
        "        update_full_maps(single_patch_prob, single_patch_prediction, i, j)\n",
        "\n",
        "\n",
        "# Export to Google Drive\n",
        "drive_path = \"/content/drive/MyDrive/Bachelorarbeit_Hannes_Grünbeck/Ergebnisse_SAM/\"\n",
        "\"\"\"\n",
        "# Original image (if needed)\n",
        "with rasterio.open(drive_path + 'original_image1.tif', 'w', **metadata_list[0]) as dst:\n",
        "    dst.write(large_test_image, 1)\n",
        "\"\"\"\n",
        "# Probability map\n",
        "with rasterio.open(drive_path + 'probability_map_full1.tif', 'w',\n",
        "                   driver='GTiff', width=width, height=height, count=1,\n",
        "                   dtype=prob_map_full.dtype, crs=crs, transform=transform) as dst:\n",
        "    dst.write(prob_map_full, 1)\n",
        "\n",
        "# Prediction mask\n",
        "with rasterio.open(drive_path + 'prediction_mask_full1.tif', 'w',\n",
        "                   driver='GTiff', width=width, height=height, count=1,\n",
        "                   dtype=pred_mask_full.dtype, crs=crs, transform=transform) as dst:\n",
        "    dst.write(pred_mask_full, 1)"
      ],
      "metadata": {
        "id": "4p-eLiW6hhgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "from rasterio.transform import Affine\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "###########################################\n",
        "###  Iterate and export img, mask and   ###\n",
        "###  prediction for ALL items in stack  ###\n",
        "###########################################\n",
        "\n",
        "# Function to update the full maps with a patch\n",
        "def update_full_maps(patch_prob, patch_mask, row, col):\n",
        "    prob_map_full[row * 256:(row + 1) * 256, col * 256:(col + 1) * 256] = patch_prob\n",
        "    pred_mask_full[row * 256:(row + 1) * 256, col * 256:(col + 1) * 256] = patch_mask\n",
        "\n",
        "# Process each image in the stack\n",
        "for image_index in range(large_test_images.shape[0]):\n",
        "    large_test_image = large_test_images[image_index]\n",
        "\n",
        "    # Get CRS and transform data from the metadata of the CURRENT image\n",
        "    crs = metadata_list[image_index]['crs']\n",
        "    transform = metadata_list[image_index]['transform']\n",
        "    height, width = large_test_image.shape\n",
        "\n",
        "    # Patchify the current image\n",
        "    patches = patchify(large_test_image, (256, 256), step=256)\n",
        "\n",
        "    # Initialize full-size arrays for probability map and prediction mask\n",
        "    prob_map_full = np.zeros_like(large_test_image, dtype=np.float32)\n",
        "    pred_mask_full = np.zeros_like(large_test_image, dtype=np.uint8)\n",
        "\n",
        "    # Iterate through all patches and make predictions\n",
        "    for i in range(patches.shape[0]):\n",
        "        for j in range(patches.shape[1]):\n",
        "            random_array = patches[i, j]\n",
        "            single_patch = Image.fromarray(random_array)\n",
        "\n",
        "            # Model prediction for the current patch (same as before)\n",
        "            #inputs = processor(single_patch.convert(\"RGB\"), input_points=input_points, return_tensors=\"pt\")\n",
        "            inputs = processor(single_patch.convert(\"RGB\"), return_tensors=\"pt\")\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            my_mito_model.eval()\n",
        "            with torch.no_grad():\n",
        "                outputs = my_mito_model(**inputs, multimask_output=False)\n",
        "\n",
        "            # Process outputs (same as before)\n",
        "            single_patch_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
        "            single_patch_prob = single_patch_prob.cpu().numpy().squeeze()\n",
        "            single_patch_prediction = (single_patch_prob > 0.5).astype(np.uint8)\n",
        "\n",
        "            # Update the full-size maps with the patch results\n",
        "            update_full_maps(single_patch_prob, single_patch_prediction, i, j)\n",
        "\n",
        "    # Export to Google Drive, including image_index in filenames\n",
        "    drive_path = \"/content/drive/MyDrive/Bachelorarbeit_Hannes_Grünbeck/Ergebnisse_SAM/\"\n",
        "\n",
        "    with rasterio.open('/content/drive/MyDrive/Bachelorarbeit_Hannes_Grünbeck/Ergebnisse_SAM/original_img/' + f'original_image_{image_index + 1}.tif', 'w', **metadata_list[image_index]) as dst:\n",
        "        dst.write(large_test_image, 1)  # Writing only the first band\n",
        "\n",
        "    with rasterio.open('/content/drive/MyDrive/Bachelorarbeit_Hannes_Grünbeck/Ergebnisse_SAM/probability/' + f'probability_{image_index + 1}.tif', 'w',\n",
        "                      driver='GTiff', width=width, height=height, count=1,\n",
        "                      dtype=prob_map_full.dtype, crs=crs, transform=transform) as dst:\n",
        "        dst.write(prob_map_full, 1)\n",
        "\n",
        "    with rasterio.open('/content/drive/MyDrive/Bachelorarbeit_Hannes_Grünbeck/Ergebnisse_SAM/mask/' + f'mask_{image_index + 1}.tif', 'w',\n",
        "                      driver='GTiff', width=width, height=height, count=1,\n",
        "                      dtype=pred_mask_full.dtype, crs=crs, transform=transform) as dst:\n",
        "        dst.write(pred_mask_full, 1)"
      ],
      "metadata": {
        "id": "O0tJp6Hhmv3g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}