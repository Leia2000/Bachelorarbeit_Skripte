{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leia2000/Bachelorarbeit_Skripte/blob/main/train_SAM_with_TIFF_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3ZVeQiT4Zq3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NSkTx6N-Mtyd"
      },
      "outputs": [],
      "source": [
        "# Install the required libraries\n",
        "#SAM\n",
        "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
        "#Transformers\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "#Datasets to prepare data and monai if you want to use special loss functions\n",
        "!pip install datasets\n",
        "!pip install -q monai\n",
        "#Patchify to divide large images into smaller patches for training. (Not necessary for smaller images)\n",
        "!pip install patchify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTXUX7xyCEGT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tifffile\n",
        "import os\n",
        "#from patchify import patchify  #Only to handle large images\n",
        "import random\n",
        "from scipy import ndimage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2B2KjUVkGMYM"
      },
      "outputs": [],
      "source": [
        "# Load tiff stack images and masks\n",
        "\n",
        "#1318 large images as tiff image stack\n",
        "large_images = tifffile.imread(\"/content/drive/MyDrive/Bachelorarbeit_Hannes_Grünbeck/No_NaN/masks/really_stacked_images.tif\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "large_images = np.load(\"/content/drive/MyDrive/Bachelorarbeit_Hannes_Grünbeck/No_NaN/masks/really_stacked_images.tif.npy\")"
      ],
      "metadata": {
        "id": "4KwFBfkK5Rai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1318 large images as tiff image stack\n",
        "large_masks = tifffile.imread(\"/content/drive/MyDrive/Bachelorarbeit_Hannes_Grünbeck/No_NaN/masks/mask_stack.tif\")\n"
      ],
      "metadata": {
        "id": "Na3smAtmaCrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(min(5, large_images.shape[0])):  # Display up to 5 slices\n",
        "    plt.figure()\n",
        "    plt.title(f\"Slice {i+1}\")\n",
        "    plt.imshow(large_images[i], cmap='gray')\n",
        "    plt.colorbar()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "IoIx_20ejFBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwN2LFUX59TM"
      },
      "outputs": [],
      "source": [
        "large_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbENtOpbbb7q"
      },
      "outputs": [],
      "source": [
        "#Desired patch size for smaller images and step size.\n",
        "patch_size = 256\n",
        "step = 256"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Function to display an image and its corresponding mask\n",
        "def display_image_mask_pair(large_images, large_masks, index):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "    # Display the image\n",
        "    axes[0].imshow(large_images, cmap='gray')\n",
        "    axes[0].set_title(f\"Image {index}\")\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Display the mask\n",
        "    axes[1].imshow(large_masks, cmap='gray')\n",
        "    axes[1].set_title(f\"Mask {index}\")\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Number of image-mask pairs to display (adjust as needed)\n",
        "num_samples_to_display = 5\n",
        "\n",
        "# Randomly select indices for display\n",
        "random_indices = np.random.choice(len(large_images), num_samples_to_display, replace=False)\n",
        "\n",
        "# Display the selected image-mask pairs\n",
        "for index in random_indices:\n",
        "    display_image_mask_pair(large_images[index], large_masks[index], index)\n",
        "\n",
        "# Print some statistics about the masks\n",
        "print(\"Mask data type:\", large_masks.dtype)\n",
        "print(\"Minimum mask value:\", large_masks.min())\n",
        "print(\"Maximum mask value:\", large_masks.max())\n",
        "print(\"Number of unique values in masks:\", len(np.unique(large_masks)))\n",
        "\n",
        "# Check for NaN or infinite values in masks\n",
        "if np.any(np.isnan(large_masks)) or np.any(np.isinf(large_masks)):\n",
        "    print(\"Warning: NaN or infinite values found in masks!\")"
      ],
      "metadata": {
        "id": "JfyHt1bPfXX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HwyzYG9KHqq"
      },
      "outputs": [],
      "source": [
        "all_img_patches = []\n",
        "for img in range(large_images.shape[0]):\n",
        "    large_image = large_images[img]\n",
        "    patches_img = patchify(large_image, (patch_size, patch_size), step=step)  #Step=256 for 256 patches means no overlap\n",
        "\n",
        "    for i in range(patches_img.shape[0]):\n",
        "        for j in range(patches_img.shape[1]):\n",
        "\n",
        "            single_patch_img = patches_img[i,j,:,:]\n",
        "            all_img_patches.append(single_patch_img)\n",
        "\n",
        "images = np.array(all_img_patches)\n",
        "\n",
        "#Let us do the same for masks\n",
        "all_mask_patches = []\n",
        "for img in range(large_masks.shape[0]):\n",
        "    large_mask = large_masks[img]\n",
        "    patches_mask = patchify(large_mask, (patch_size, patch_size), step=step)  #Step=256 for 256 patches means no overlap\n",
        "\n",
        "    for i in range(patches_mask.shape[0]):\n",
        "        for j in range(patches_mask.shape[1]):\n",
        "\n",
        "            single_patch_mask = patches_mask[i,j,:,:]\n",
        "            single_patch_mask = (single_patch_mask / 255.).astype(np.uint8)\n",
        "            all_mask_patches.append(single_patch_mask)\n",
        "\n",
        "masks = np.array(all_mask_patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6yCILU0DvU8"
      },
      "outputs": [],
      "source": [
        "images.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Function to display an image and its corresponding mask\n",
        "def display_image_mask_pair(image, mask, index):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "    # Display the image\n",
        "    axes[0].imshow(image, cmap='gray')\n",
        "    axes[0].set_title(f\"Image {index}\")\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Display the mask\n",
        "    axes[1].imshow(mask, cmap='gray')\n",
        "    axes[1].set_title(f\"Mask {index}\")\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Number of image-mask pairs to display (adjust as needed)\n",
        "num_samples_to_display = 5\n",
        "\n",
        "# Randomly select indices for display\n",
        "random_indices = np.random.choice(len(images), num_samples_to_display, replace=False)\n",
        "\n",
        "# Display the selected image-mask pairs\n",
        "for index in random_indices:\n",
        "    display_image_mask_pair(images[index], masks[index], index)\n",
        "\n",
        "# Print some statistics about the masks\n",
        "print(\"Mask data type:\", masks.dtype)\n",
        "print(\"Minimum mask value:\", masks.min())\n",
        "print(\"Maximum mask value:\", masks.max())\n",
        "print(\"Number of unique values in masks:\", len(np.unique(masks)))\n",
        "\n",
        "# Check for NaN or infinite values in masks\n",
        "if np.any(np.isnan(masks)) or np.any(np.isinf(masks)):\n",
        "    print(\"Warning: NaN or infinite values found in masks!\")"
      ],
      "metadata": {
        "id": "t0FSvLfifHn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list to store the indices of non-empty masks\n",
        "valid_indices = [i for i, mask in enumerate(masks) if mask.max() != 0]\n",
        "# Filter the image and mask arrays to keep only the non-empty pairs\n",
        "filtered_images = images[valid_indices]\n",
        "filtered_masks = masks[valid_indices]\n",
        "print(\"Image shape:\", filtered_images.shape)  # e.g., (num_frames, height, width, num_channels)\n",
        "print(\"Mask shape:\", filtered_masks.shape)"
      ],
      "metadata": {
        "id": "c9z-yCrWeqen",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFeELFDdOjgz"
      },
      "outputs": [],
      "source": [
        "all_img_patches = []\n",
        "for img in range(large_images.shape[0]):\n",
        "    large_image = large_images[img]\n",
        "\n",
        "    # Check if the image is already the size of a single patch\n",
        "    if large_image.shape[0] == patch_size and large_image.shape[1] == patch_size:\n",
        "        # If yes, append the image directly without patching\n",
        "        all_img_patches.append(large_image)\n",
        "    else:\n",
        "        # If not, proceed with patching\n",
        "        # patchify expects a 2D or 3D input: (height, width) or (height, width, channels)\n",
        "        # Assuming large_image is (height, width, channels), proceed directly\n",
        "        patches_img = patchify(large_image, (patch_size, patch_size, large_images.shape[2]), step=step)\n",
        "\n",
        "        for i in range(patches_img.shape[0]):\n",
        "            for j in range(patches_img.shape[1]):\n",
        "                single_patch_img = patches_img[i, j, 0, :, :, :] # Extract the patch from the 5D output\n",
        "                all_img_patches.append(single_patch_img)\n",
        "\n",
        "images = np.array(all_img_patches)\n",
        "\n",
        "# Apply the same logic to masks, but without channels\n",
        "all_mask_patches = []\n",
        "for img in range(large_masks.shape[0]):\n",
        "    large_mask = large_masks[img]\n",
        "\n",
        "    if large_mask.shape[0] == patch_size and large_mask.shape[1] == patch_size:\n",
        "        all_mask_patches.append(large_mask)\n",
        "    else:\n",
        "        # patchify for masks should only consider 2 dimensions (height, width)\n",
        "        patches_mask = patchify(large_mask, (patch_size, patch_size), step=step)\n",
        "\n",
        "        for i in range(patches_mask.shape[0]):\n",
        "            for j in range(patches_mask.shape[1]):\n",
        "                single_patch_mask = patches_mask[i, j, :, :] # Extract the patch from the 4D output\n",
        "                single_patch_mask = (single_patch_mask / 255.).astype(np.uint8)\n",
        "                all_mask_patches.append(single_patch_mask)\n",
        "\n",
        "masks = np.array(all_mask_patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOdcxsLcPTnB"
      },
      "outputs": [],
      "source": [
        "print(\"Shape der TIF-Datei:\", large_masks.shape)\n",
        "\n",
        "# Eine Ebene der TIF-Datei anzeigen (z. B. die erste Ebene)\n",
        "plt.imshow(large_masks[0], cmap=\"gray\")  # cmap=\"gray\" für Schwarz-Weiß-Darstellung\n",
        "plt.title(\"Erste Ebene der TIF-Datei\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtswt5WRSEJz"
      },
      "outputs": [],
      "source": [
        "# Create a list to store the indices of non-empty masks\n",
        "valid_indices = [i for i, mask in enumerate(masks) if mask.max() != 0]\n",
        "# Filter the image and mask arrays to keep only the non-empty pairs\n",
        "filtered_images = images[valid_indices]\n",
        "filtered_masks = masks[valid_indices]\n",
        "print(\"Image shape:\", large_images.shape)  # e.g., (num_frames, height, width, num_channels)\n",
        "print(\"Mask shape:\", large_masks.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwBvAtrhQO9p"
      },
      "outputs": [],
      "source": [
        "# Graustufen berechnen (Mittelwert über die Kanäle)\n",
        "grayscale_images = np.mean(large_images, axis=-1)  # Mittelwert entlang der Kanalachse\n",
        "print(\"Shape Images:\", grayscale_images.shape)  # Sollte (1318, 256, 256) sein\n",
        "print(\"Shape Masks:\", large_masks.shape)  # Sollte (1318, 256, 256) sein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyxDVRvqVRMM"
      },
      "outputs": [],
      "source": [
        "print(\"large_masks dtype:\", large_masks.dtype)\n",
        "print(\"large_masks min:\", large_masks.min(), \"max:\", large_masks.max())\n",
        "\n",
        "large_masks_normalized = (large_masks / large_masks.max() * 255).astype(np.uint8)\n",
        "print(\"large_masks_normalized min:\", large_masks_normalized.min(), \"max:\", large_masks_normalized.max())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hAoOYN2_s7l"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "# Convert the NumPy arrays to Pillow images and store them in a dictionary\n",
        "dataset_dict = {\n",
        "    \"image\": [Image.fromarray(img) for img in grayscale_images],\n",
        "    \"label\": [Image.fromarray(mask) for mask in large_masks_normalized],\n",
        "}\n",
        "\n",
        "# Create the dataset using the datasets.Dataset class\n",
        "dataset = Dataset.from_dict(dataset_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcnNq5s8Q2gw"
      },
      "outputs": [],
      "source": [
        "print(\"Shape der TIF-Datei:\", large_masks.shape)\n",
        "\n",
        "# Eine Ebene der Image-Datei anzeigen (z. B. die erste Ebene)\n",
        "plt.imshow(grayscale_images[5], cmap=\"gray\")  # cmap=\"gray\" für Schwarz-Weiß-Darstellung\n",
        "plt.title(\"Erste Ebene der TIF-Datei\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# Eine Ebene der TIF-Datei anzeigen (z. B. die erste Ebene)\n",
        "plt.imshow(large_masks[5], cmap=\"gray\")  # cmap=\"gray\" für Schwarz-Weiß-Darstellung\n",
        "plt.title(\"Erste Ebene der TIF-Datei\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fISBbWMRBfV3"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAiRRfJhGmVs"
      },
      "outputs": [],
      "source": [
        "img_num = random.randint(0, grayscale_images.shape[0]-1)\n",
        "example_image = dataset[img_num][\"image\"]\n",
        "example_mask = dataset[img_num][\"label\"]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# Plot the first image on the left\n",
        "axes[0].imshow(np.array(example_image), cmap='gray')  # Assuming the first image is grayscale\n",
        "axes[0].set_title(\"Image\")\n",
        "\n",
        "# Plot the second image on the right\n",
        "axes[1].imshow(example_mask, cmap='gray')  # Assuming the second image is grayscale\n",
        "axes[1].set_title(\"Mask\")\n",
        "\n",
        "# Hide axis ticks and labels\n",
        "for ax in axes:\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "\n",
        "# Display the images side by side\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYmrZ9hWCpKV"
      },
      "outputs": [],
      "source": [
        "#Get bounding boxes from mask.\n",
        "def get_bounding_box(ground_truth_map):\n",
        "  # get bounding box from mask\n",
        "  y_indices, x_indices = np.where(ground_truth_map > 0)\n",
        "  x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
        "  y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
        "  # add perturbation to bounding box coordinates\n",
        "  H, W = ground_truth_map.shape\n",
        "  x_min = max(0, x_min - np.random.randint(0, 20))\n",
        "  x_max = min(W, x_max + np.random.randint(0, 20))\n",
        "  y_min = max(0, y_min - np.random.randint(0, 20))\n",
        "  y_max = min(H, y_max + np.random.randint(0, 20))\n",
        "  bbox = [x_min, y_min, x_max, y_max]\n",
        "\n",
        "  return bbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQVNaYYbDKf6"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SAMDataset(Dataset):\n",
        "  \"\"\"\n",
        "  This class is used to create a dataset that serves input images and masks.\n",
        "  It takes a dataset and a processor as input and overrides the __len__ and __getitem__ methods of the Dataset class.\n",
        "  \"\"\"\n",
        "  def __init__(self, dataset, processor):\n",
        "    self.dataset = dataset\n",
        "    self.processor = processor\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = self.dataset[idx]\n",
        "    image = item[\"image\"]\n",
        "    ground_truth_mask = np.array(item[\"label\"])\n",
        "\n",
        "    # get bounding box prompt\n",
        "    prompt = get_bounding_box(ground_truth_mask)\n",
        "\n",
        "    # prepare image and prompt for the model\n",
        "    inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
        "\n",
        "    # remove batch dimension which the processor adds by default\n",
        "    inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
        "\n",
        "    # add ground truth segmentation\n",
        "    inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
        "\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IVksPItDOYg"
      },
      "outputs": [],
      "source": [
        "# Initialize the processor\n",
        "from transformers import SamProcessor\n",
        "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bSlgShwNSsr"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "# Convert the NumPy arrays to Pillow images and store them in a dictionary\n",
        "dataset_dict = {\n",
        "    \"image\": [Image.fromarray(img).convert(\"RGB\") for img in grayscale_images], # Convert to RGB\n",
        "    \"label\": [Image.fromarray(mask) for mask in large_masks_normalized],\n",
        "}\n",
        "\n",
        "# Create the dataset using the datasets.Dataset class\n",
        "dataset = Dataset.from_dict(dataset_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU59Asl5DQCF"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the SAMDataset\n",
        "train_dataset = SAMDataset(dataset=dataset, processor=processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMUQO3fCDTyV"
      },
      "outputs": [],
      "source": [
        "example = train_dataset[0]\n",
        "for k,v in example.items():\n",
        "  print(k,v.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G694dS0RDWdl"
      },
      "outputs": [],
      "source": [
        "# Create a DataLoader instance for the training dataset\n",
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, drop_last=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAHAY4LxDYCt"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "for k,v in batch.items():\n",
        "  print(k,v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0G55sGADaJ3"
      },
      "outputs": [],
      "source": [
        "batch[\"ground_truth_mask\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fUTkHq3DcVV"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "from transformers import SamModel\n",
        "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
        "\n",
        "# make sure we only compute gradients for mask decoder\n",
        "for name, param in model.named_parameters():\n",
        "  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
        "    param.requires_grad_(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUQFybjODebi"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "import monai\n",
        "# Initialize the optimizer and the loss function\n",
        "optimizer = Adam(model.mask_decoder.parameters(), lr=1e-5, weight_decay=0)\n",
        "#Try DiceFocalLoss, FocalLoss, DiceCELoss\n",
        "seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XbD2PQlPDgFF"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from statistics import mean\n",
        "import torch\n",
        "from torch.nn.functional import threshold, normalize\n",
        "\n",
        "#Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_losses = []\n",
        "    for batch in tqdm(train_dataloader):\n",
        "      # forward pass\n",
        "      outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
        "                      input_boxes=batch[\"input_boxes\"].to(device),\n",
        "                      multimask_output=False)\n",
        "\n",
        "      # compute loss\n",
        "      predicted_masks = outputs.pred_masks.squeeze(1)\n",
        "      ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
        "      loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
        "\n",
        "      # backward pass (compute gradients of parameters w.r.t. loss)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      # optimize\n",
        "      optimizer.step()\n",
        "      epoch_losses.append(loss.item())\n",
        "\n",
        "    print(f'EPOCH: {epoch}')\n",
        "    print(f'Mean loss: {mean(epoch_losses)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zm89g_tbcdN"
      },
      "outputs": [],
      "source": [
        "# Save the model's state dictionary to a file\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/Bachelorarbeit_Hannes_Grünbeck/Ergebnisse_SAM/test.pth\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}